{
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "lastEditStatus": {
   "notebookId": "agvbgwzebpcgyca5sijw",
   "authorId": "7488247245231",
   "authorName": "CAMEL",
   "authorEmail": "s10256056@connect.np.edu.sg",
   "sessionId": "44d81a81-ba1c-46ab-af5e-02a6e165fee9",
   "lastEditTime": 1737339657078
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3d0f27-beda-4bd8-ba62-f254495e7199",
   "metadata": {
    "collapsed": false,
    "name": "cell11",
    "resultHeight": 155
   },
   "source": [
    "#### Why Snowpark?\n",
    "The purpose of Snowpark lies in its ability to provide more flexibility, scalability, and integration for data processing and orchestration tasks.|\n",
    "\n",
    "Orchestrating Jobs and Pipelines: We can automate Snowpark-based pipelines (UDFs, views, and other transformations) in a more flexible way. Snowpark code can be versioned and tested, making it easier to maintain and extend in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033f21d-2a21-498f-9d92-a809a24dd226",
   "metadata": {
    "name": "cell22",
    "collapsed": false,
    "resultHeight": 153
   },
   "source": "## TO NEHAA:\n### GO TO CELL 1\n### GET TO WORK NOW"
  },
  {
   "cell_type": "markdown",
   "id": "2742fdce-c605-4862-a8f4-aa294af70c92",
   "metadata": {
    "name": "cell18",
    "collapsed": false,
    "resultHeight": 130
   },
   "source": "#### Note:\nThis notebook is to validate, clean and update data in tables of higher priority in terms of updating speed.\n\nData tables that need a faster updating speed will be processed here."
  },
  {
   "cell_type": "markdown",
   "id": "f699edd5-3e0a-4d6d-baa4-c6f035e9b53a",
   "metadata": {
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 178
   },
   "source": "#### Desired Flow\n1. Grab data from raw tables\n2. Check for data, validate, check and update cleaned data onto the cleaned tables\n3. Join tables via foreign keys and make hybrid master table\n4. Create Schema Views"
  },
  {
   "cell_type": "markdown",
   "id": "a87236c5-ff90-4956-99d6-234a7300a821",
   "metadata": {
    "collapsed": false,
    "name": "cell5",
    "resultHeight": 597
   },
   "source": "#### 1. Loading Raw Data In\n\nSince we already push our data onto Snowflake, we can call for them in this notebook to run in Snowpark. This will be the first step to the data flow overview for establishing the CI/CD deployment & finalizing the ELT pipepline.\n\n##### Tables in this notebook\n- Sales.CustomerTransactions \n- Sales.CustomerCategories \n- Application.People \n- Purchasing.SupplierTransactions \n- Purchasing.PurchaseOrderLines \n- Warehouse.ColdRoomTemperatures \n- Warehouse.VehicleTemperatures\n- Sales.Orders \n- Purchasing.PurchaseOrders\n- Warehouse.StockItemTransactions\n- Warehouse.StockItems\n- Warehouse.StockItemHoldings\n- Sales.Customers\n- Sales.Invoices\n- Sales.InvoiceLines\n- Sales.Orderlines"
  },
  {
   "cell_type": "code",
   "id": "75a8b26d-2b47-414f-894a-5e0d3dcc923d",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# RAW TABLES\n# ---------------------------------------------------------------------------------------------\nimport time\nfrom snowflake.snowpark import Session\n\n# Define all tables organized by categories or schemas (CALL FOR RAW TABLES)\nTABLE_DICT = {\n    \"application\": {\n        \"schema\": \"KN_LOGISTICS.SNOWSQL\", \n        \"tables\": [\n            \"APPLICATION_PEOPLE_RAW\"\n        ]\n    },\n    \"purchasing\": {\n        \"schema\": \"KN_LOGISTICS.SNOWSQL\",\n        \"tables\": [\n            \"PURCHASING_PURCHASEORDERLINES_RAW\",\n            \"PURCHASING_PURCHASEORDERS_RAW\"\n        ]\n    },\n    \"sales\": {\n        \"schema\": \"KN_LOGISTICS.SNOWSQL\",\n        \"tables\": [\n            \"SALES_CUSTOMERCATEGORIES_RAW\",\n            \"SALES_CUSTOMERS_RAW\",\n            \"SALES_CUSTOMERTRANSACTIONS_RAW\",\n            \"SALES_INVOICELINES_RAW\",\n            \"SALES_INVOICES_RAW\",\n            \"SALES_ORDERLINES_RAW\",\n            \"SALES_ORDERS_RAW\"\n        ]\n    },\n    \"warehouse\": {\n        \"schema\": \"KN_LOGISTICS.SNOWSQL\",\n        \"tables\": [\n            \"WAREHOUSE_COLDROOMTEMPERATURES_RAW\",\n            \"WAREHOUSE_STOCKITEMHOLDINGS_RAW\",\n            \"WAREHOUSE_STOCKITEMS_RAW\",\n            \"WAREHOUSE_STOCKITEMTRANSACTIONS_RAW\",\n            \"WAREHOUSE_VEHICLETEMPERATURES_RAW\"\n        ]\n    }\n}\n\ndef load_raw_table(session, tname=None, schema=None):\n    # Adjusted for direct use (no S3 staging assumed in your case)\n    session.use_schema(schema)\n    print(f\"Loading table: {tname}\")\n    # If additional logic for transformations/loading is needed, add it here\n    df = session.table(tname)\n    df.show()  # Example action to verify table content\n\ndef load_all_tables(session):\n    for category, data in TABLE_DICT.items():\n        schema = data['schema']\n        tables = data['tables']\n        for tname in tables:\n            load_raw_table(session, tname=tname, schema=schema)\n\ndef validate_tables(session):\n    for category, data in TABLE_DICT.items():\n        schema = data['schema']\n        tables = data['tables']\n        for tname in tables:\n            session.use_schema(schema)\n            print(f\"Validating table: {tname}\")\n            print(f\"Columns: {session.table(tname).columns}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32660d9-0f5c-4975-afd2-bb8b3cacb3ad",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell6",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Add the utils package to our path and import the snowpark_utils function\n",
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e838c-495c-49df-94be-d548c81adbb7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell7",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "#from snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb151bf-2d29-43d7-9a65-d08dac6c7064",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell8",
    "resultHeight": 4924
   },
   "outputs": [],
   "source": [
    "load_all_tables(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c36df-1fe8-4bcf-9efb-e834b978efb0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell9",
    "resultHeight": 693
   },
   "outputs": [],
   "source": [
    "validate_tables(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98315f80-1db2-4e4c-b2f9-7574848b3e37",
   "metadata": {
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 379
   },
   "source": "#### 2. Check for data, validate, check and update cleaned data onto the cleaned tables\nAfter pulling in the raw data, we can check and validate these raw data to make sure they are of a certain format eligible to be pushed over to the cleaned tables. \n\nIf not, we will clean the tables accordingly then update them over to the cleaned tables.\n\nSome of the validation we can do is:\n- Check for null values\n- Check for duplicates in PK and UQ\n- Check for invalid datetypes & text formats\n\nAfter cleaning the data, we need to validate ONE MORE TIME to make sure the data has not been imported into the cleaned tables before. If validation succeeds, we update the current records over to the cleaned tables.\n- Check if the data records exist in the cleaned tables (checkj if they are identitcal)"
  },
  {
   "cell_type": "code",
   "id": "fb0e44bd-bde2-4e11-a9e6-f2f0fb63b37c",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "TABLE_CONFIG = {\n    \"APPLICATION_PEOPLE_RAW\": {\n        \"clean_table\": \"APPLICATION_PEOPLE\",\n        \"primary_keys\": [\"PERSON_ID\"],\n        \"unique_keys\": [],\n        \"date_format\": \"%Y-%m-%d\"\n    },\n    \"PURCHASING_SUPPLIERTRANSACTIONS_RAW\": {\n        \"clean_table\": \"PURCHASING_SUPPLIERTRANSACTIONS\",\n        \"primary_keys\": [\"SUPPLIERTRANSACTIONID\"],\n        \"unique_keys\": [],\n        \"date_format\": \"%Y-%m-%d\"\n    },\n    \"PURCHASING_PURCHASEORDERS_RAW\": {\n        \"clean_table\": \"PURCHASING_PURCHASEORDERS\",\n        \"primary_keys\": [\"PURCHASEORDERID\"],\n        \"unique_keys\": [],\n        \"date_format\": \"%Y-%m-%d\"\n    },\n    # Add all tables here...\n}\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a8f2ee0-aaa1-47f4-8cb3-f413bd1d2747",
   "metadata": {
    "language": "python",
    "name": "cell20",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\n\ndef clean_table(session: Session, raw_table: str, config: dict):\n    \"\"\"\n    Cleans a raw table based on the provided configuration.\n\n    Args:\n        session (Session): Snowflake Snowpark session.\n        raw_table (str): Fully qualified name of the raw table (schema.table).\n        config (dict): Configuration for the table.\n\n    Returns:\n        DataFrame: Cleaned DataFrame.\n    \"\"\"\n    print(f\"Cleaning table: {raw_table}\")\n    df = session.table(raw_table)\n\n    # Replace 'NULL' strings with actual NULL values\n    cleaned_df = df.replace({\"NULL\": None})\n\n    # Handle date columns\n    if \"date_format\" in config:\n        for column in df.columns:\n            cleaned_df = cleaned_df.with_column(column, col(column).cast(\"date\"))\n\n    # Drop duplicates based on primary keys\n    if \"primary_keys\" in config:\n        cleaned_df = cleaned_df.drop_duplicates(subset=config[\"primary_keys\"])\n\n    return cleaned_df\n\ndef add_constraints(session: Session, clean_table: str, config: dict):\n    \"\"\"\n    Adds primary and unique key constraints to the cleaned table.\n\n    Args:\n        session (Session): Snowflake Snowpark session.\n        clean_table (str): Fully qualified name of the cleaned table (schema.table).\n        config (dict): Configuration for the table.\n    \"\"\"\n    # Add primary key\n    if \"primary_keys\" in config:\n        primary_keys = \", \".join(config[\"primary_keys\"])\n        session.sql(f\"\"\"\n            ALTER TABLE {clean_table}\n            ADD CONSTRAINT PK_{clean_table.replace('.', '_')} PRIMARY KEY ({primary_keys})\n        \"\"\").collect()\n\n    # Add unique keys\n    if \"unique_keys\" in config and config[\"unique_keys\"]:\n        unique_keys = \", \".join(config[\"unique_keys\"])\n        session.sql(f\"\"\"\n            ALTER TABLE {clean_table}\n            ADD CONSTRAINT UK_{clean_table.replace('.', '_')} UNIQUE ({unique_keys})\n        \"\"\").collect()\n\n    print(f\"Constraints added to {clean_table}\")\n\n\ndef process_table(session: Session, raw_table: str, config: dict):\n    \"\"\"\n    Cleans and processes a raw table based on the provided configuration.\n\n    Args:\n        session (Session): Snowflake Snowpark session.\n        raw_table (str): Fully qualified name of the raw table (schema.table).\n        config (dict): Configuration for the table.\n    \"\"\"\n    clean_table_name = config[\"clean_table\"]\n\n    # Step 1: Clean the raw table\n    cleaned_df = clean_table(session, raw_table, config)\n\n    # Step 2: Save the cleaned data as a new table\n    cleaned_df.write.mode(\"overwrite\").save_as_table(clean_table_name)\n    print(f\"Cleaned table created: {clean_table_name}\")\n\n    # Step 3: Add constraints to the cleaned table\n    add_constraints(session, clean_table_name, config)\n    print(f\"Processing complete for table: {raw_table}\")\n\n\ndef process_all_tables(session: Session, table_config: dict):\n    \"\"\"\n    Processes all tables based on the configuration and logs the status.\n\n    Args:\n        session (Session): Snowflake Snowpark session.\n        table_config (dict): Configuration dictionary for all tables.\n\n    Returns:\n        dict: Summary of success and failure for each table.\n    \"\"\"\n    # Initialize status tracking\n    status_summary = {\"success\": [], \"failure\": []}\n\n    for raw_table, config in table_config.items():\n        try:\n            # Process the table\n            print(f\"Processing table: {raw_table}\")\n            process_table(session, f\"KN_LOGISTICS.SNOW_SQL.{raw_table}\", config)\n\n            # Log success\n            print(f\"Successfully processed table: {raw_table}\")\n            status_summary[\"success\"].append(raw_table)\n\n        except Exception as e:\n            # Log failure and exception details\n            print(f\"Failed to process table: {raw_table}. Error: {str(e)}\")\n            status_summary[\"failure\"].append({\"table\": raw_table, \"error\": str(e)})\n\n    # Return status summary\n    return status_summary\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56684b13-8424-421f-b1b0-df3c0c779c29",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "codeCollapsed": false,
    "resultHeight": 492
   },
   "outputs": [],
   "source": "# Process all tables\nstatus_summary = process_all_tables(session, TABLE_CONFIG)\n\n# Print summary\nprint(\"\\nProcessing Summary:\")\nprint(f\"Successful Tables: {len(status_summary['success'])}\")\nfor table in status_summary[\"success\"]:\n    print(f\"  - {table}\")\n\nprint(f\"\\nFailed Tables: {len(status_summary['failure'])}\")\nfor failure in status_summary[\"failure\"]:\n    print(f\"  - Table: {failure['table']}, Error: {failure['error']}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba1fe6a8-428f-4ebb-8eff-e6848b35b6ed",
   "metadata": {
    "collapsed": false,
    "name": "cell10",
    "resultHeight": 223
   },
   "source": "#### 4. Create SNOWSQL View\nWe will simplify the SNOWSQL schema by joining together the tables and picking only the columns we need. This will be done using Snowpark Dataframe API. Then we'll create a Snowflake stream on that view so that we can incrementally process changes to any of the SNOWSQL tables.\n\nThis setup is crucial for ensuring that once the initial data is loaded, we can efficiently manage incremental updates to the SNOWSQL data through the Snowflake stream.\n\nThis joining of tables can only be done when the group has finalized cleaning the tables in Snowflake (Snowsight)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af2036-c048-4b0e-8c76-c882e63c1a6c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# SNOWFLAKE ADVANTAGE: Snowpark DataFrame API\n# SNOWFLAKE ADVANTAGE: Streams for incremental processing (CDC)\n# SNOWFLAKE ADVANTAGE: Streams on views\n\n### I NEED HELP ON JOINING THE DATA HERE!!!!!!\n# ---------------------------------------------------------------------------------------------\n\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.functions import col\n\ndef create_pos_view(session):\n    session.use_schema('SNOWSQL')\n\n    # Define DataFrames for each table with selected column\n    application_people = session.table(\"KN_LOGISTICS.SNOWSQL.APPLICATION_PEOPLE\").select(\n        F.col(\"PERSONID\"),\n        F.col(\"FULLNAME\"),\n        F.col(\"ISEMPLOYEE\"),\n        F.col(\"ISSALESPERSON\"),\n        F.col(\"PREFERREDNAME\"),\n        F.col(\"SEARCHNAME\")\n    )\n\n    purchasing_purchaseorderlines = session.table(\"KN_LOGISTICS.SNOWSQL.PURCHASING_PURCHASEORDERLINES\").select(\n        F.col(\"DESCRIPTION\"),\n        F.col(\"EXPECTEDUNITPRICEPEROUTER\"),\n        F.col(\"ISORDERLINEFINALIZED\"),\n        F.col(\"LASTRECEIPTDATE\"),\n        F.col(\"ORDERDOUTERS\"),\n        F.col(\"PACKAGETYPEID\"),\n        F.col(\"PURCHASEORDERID\"),\n        F.col(\"PURCHASEORDERLINEID\"),\n        F.col(\"RECEIVEDOUTERS\"),\n        F.col(\"STOCKITEMID\")\n    )\n\n    purchasing_purchaseorders = session.table(\"KN_LOGISTICS.SNOWSQL.PURCHASING_PURCHASEORDERS\").select(\n        F.col(\"CONTACTPERSONID\"),\n        F.col(\"DELIVERYMETHODID\"),\n        F.col(\"EXPECTEDDELIVERYDATE\"),\n        F.col(\"ISORDERFINALIZED\"),\n        F.col(\"ORDERDATE\"),\n        F.col(\"PURCHASEORDERID\"),\n        F.col(\"SUPPLIERID\"),\n        F.col(\"SUPPLIERREFERENCE\")\n    )\n\n    purchasing_suppliertransactions = session.table(\"KN_LOGISTICS.SNOWSQL.PURCHASING_SUPPLIERTRANSACTIONS\").select(\n        F.col(\"AMOUNTEXCLUDINGTAX\"),\n        F.col(\"FINALIZATIONDATE\"),\n        F.col(\"ISFINALIZED\"),\n        F.col(\"LASTCOSTPRICE_NUM\"),\n        F.col(\"OUTSTANDINGBALANCE\"),\n        F.col(\"PAYMENTMETHODID\"),\n        F.col(\"PURCHASEORDERID\"),\n        F.col(\"SUPPLIERID\"),\n        F.col(\"SUPPLIERINVOICENUMBER\"),\n        F.col(\"SUPPLIERTRANSACTIONID\"),\n        F.col(\"TAXAMOUNT\"),\n        F.col(\"TRANSACTIONAMOUNT\"),\n        F.col(\"TRANSACTIONDATE\"),\n        F.col(\"TRANSACTIONTYPEID\")\n    )\n\n    sales_customercategories = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_CUSTOMERCATEGORIES\").select(\n        F.col(\"CUSTOMERCATEGORYID\"),\n        F.col(\"CUSTOMERCATEGORYNAME\")\n    )\n\n    \n    sales_customers = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_CUSTOMERS\").select(\n        F.col(\"CUSTOMERID\"),\n        F.col(\"CUSTOMERNAME\"),\n        F.col(\"BILLTOCUSTOMERID\"),\n        F.col(\"CUSTOMERCATEGORYID\"),\n        F.col(\"BUYINGGROUPID\"),\n        F.col(\"PRIMARYCONTACTPERSONID\"),\n        F.col(\"ALTERNATECONTACTPERSONID\"),\n        F.col(\"DELIVERYMETHODID\"),\n        F.col(\"DELIVERYCITYID\"),\n        F.col(\"CREDITLIMIT\"),\n        F.col(\"ACCOUNTOPENEDDATE\"),\n        F.col(\"STANDARDDISCOUNTPERCENTAGE\"),\n        F.col(\"ISSTATEMENTSENT\"),\n        F.col(\"ISONCREDITHOLD\"),\n        F.col(\"PAYMENTDAYS\"),\n        F.col(\"PHONENUMBER\"),\n        F.col(\"WEBSITEURL\")\n    )\n\n    sales_customertransactions = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_CUSTOMERTRANSACTIONS\").select(\n        F.col(\"AMOUNTEXCLUDINGTAX\"),\n        F.col(\"CUSTOMERID\"),\n        F.col(\"CUSTOMERTRANSACTIONID\"),\n        F.col(\"FINALIZATIONDATE\"),\n        F.col(\"INVOICEID\"),\n        F.col(\"ISFINALIZED\"),\n        F.col(\"OUTSTANDINGBALANCE\"),\n        F.col(\"PAYMENTMETHODID\"),\n        F.col(\"TAXAMOUNT\"),\n        F.col(\"TRANSACTIONAMOUNT\"),\n        F.col(\"TRANSACTIONDATE\"),\n        F.col(\"TRANSACTIONTYPEID\")\n    )\n\n    sales_invoicelines = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_INVOICELINES\").select(\n        F.col(\"DESCRIPTION\"),\n        F.col(\"EXTENDEDPRICE\"),\n        F.col(\"INVOICEID\"),\n        F.col(\"INVOICELINEID\"),\n        F.col(\"LINEPROFIT\"),\n        F.col(\"PACKAGETYPEID\"),\n        F.col(\"QUANTTY\"),\n        F.col(\"STOCKITEMID\"),\n        F.col(\"TAXAMOUNT\"),\n        F.col(\"TAXRATE\"),\n        F.col(\"UNITPRICE\")\n    )\n\n    sales_invoices = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_INVOICES\").select(\n        F.col(\"ACCOUNTSPERSONID\"),\n        F.col(\"BILLTOCUSTOMERID\"),\n        F.col(\"CONFIRMEDDELIVERYTIME\"),\n        F.col(\"CONFIRMEDRECEIVEDBY\"),\n        F.col(\"CONTACTPERSONID\"),\n        F.col(\"CUSTOMERID\"),\n        F.col(\"CUSTOMERPURCHASEORDERNUMBER\"),\n        F.col(\"DELIVERYMETHODID\"),\n        F.col(\"INVOICEDATE\"),\n        F.col(\"INVOICEID\"),\n        F.col(\"ORDERID\"),\n        F.col(\"PACKEDBYPERSONID\"),\n        F.col(\"SALESPERSONPERSONID\"),\n        F.col(\"TOTALCHILLERITEMS\"),\n        F.col(\"TOTALDRYITEMS\")\n    )\n\n    sales_orderlines = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_ORDERLINES\").select(\n        F.col(\"DESCRIPTION\"),\n        F.col(\"ORDERID\"),\n        F.col(\"ORDERLINEID\"),\n        F.col(\"PACKAGETYPEID\"),\n        F.col(\"PICKEDQUANTITY\"),\n        F.col(\"PICKINGCOMPLETEDWHEN\"),\n        F.col(\"QUANTITY\"),\n        F.col(\"STOCKITEMID\"),\n        F.col(\"TAXRATE\"),\n        F.col(\"UNITPRICE\")\n    )\n\n    sales_ordes = session.table(\"KN_LOGISTICS.SNOWSQL.SALES_ORDERS\").select(\n        F.col(\"CONTACTPERSONID\"),\n        F.col(\"CUSTOMERID\"),\n        F.col(\"CUSTOMERPURCHASEORDERNUMBER\"),\n        F.col(\"EXPECTEDDELIVERYDATE\"),\n        F.col(\"ISUNDERSUPPLYBACKORDERED\"),\n        F.col(\"ORDERDATE\"),\n        F.col(\"ORDERID\"),\n        F.col(\"PICKEDBYPERSONID\"),\n        F.col(\"PICKINGCOMPLETEDWHEN\"),\n        F.col(\"SALESPERSONPERSONID\")\n    )\n\n    warehouse_coldroomtemperatures = session.table(\"KN_LOGISTICS.SNOWSQL.WAREHOUSE_COLDROOMTEMPERATURES\").select(\n        F.col(\"COLDROOMSENSORNUMBER\"),\n        F.col(\"COLDROOMTEMPERATUREID\"),\n        F.col(\"RECORDEDWHEN\"),\n        F.col(\"RECORDEDWHEN_T\"),\n        F.col(\"RECORDEDWHEN_TS\"),\n        F.col(\"TEMPERATURE\"),\n        F.col(\"TEMPERATURE_FLOAT\")\n    )\n\n    warehouse_stockitemholdings = session.table(\"KN_LOGISTICS.SNOWSQL.WAREHOUSE_STOCKITEMHOLDINGS\").select(\n        F.col(\"BINLOCATION\"),\n        F.col(\"LASTCOSTPRICE\"),\n        F.col(\"LASTSTOCKTAKEQUANTITY\"),\n        F.col(\"QUANTITYONHAND\"),\n        F.col(\"REORDERLEVEL\"),\n        F.col(\"STOCKITEMID\"),\n        F.col(\"TARGETSTOCKLEVEL\")\n    )\n    \n    warehouse_stockitems = session.table(\"KN_LOGISTICS.SNOWSQL.WAREHOUSE_STOCKITEMS\").select(\n        F.col(\"STOCKITEMID\"),\n        F.col(\"STOCKITEMNAME\"),\n        F.col(\"UNITPACKAGEID\"),\n        F.col(\"OUTERPACKAGEID\"),\n        F.col(\"BRAND\"),\n        F.col(\"SIZE\"),\n        F.col(\"LEADTIMEDAYS\"),\n        F.col(\"QUANTITYPEROUTER\"),\n        F.col(\"ISCHILLERSTOCK\"),\n        F.col(\"TAXRATE\"),\n        F.col(\"UNITPRICE\"),\n        F.col(\"RECOMMENDEDRETAILPRICE\"),\n        F.col(\"TYPICALWEIGHTPERUNIT\"),\n        F.col(\"SUPPLIERID\"),\n        F.col(\"COLORID\")\n    )\n\n    warehouse_stockitemtransactions = session.table(\"KN_LOGISTICS.SNOWSQL.WAREHOUSE_STOCKITEMTRANSACTIONS\").select(\n        F.col(\"CUSTOMERID\"),\n        F.col(\"INVOICEID\"),\n        F.col(\"PURCHASEORDERID\"),\n        F.col(\"QUANTITY\"),\n        F.col(\"STOCKITEMID\"),\n        F.col(\"STOCKITEMTRANSACTIONID\"),\n        F.col(\"SUPPLIERID\"),\n        F.col(\"TRANSACTIONOCCURREDWHEN\"),\n        F.col(\"TRANSACTIONTYPEID\")\n    )\n\n    warehouse_vehicletemperatures = session.table(\"KN_LOGISTICS.SNOWSQL.WAREHOUSE_VEHICLETEMPERATURES\").select(\n        F.col(\"CHILLERSENSORNUMBER\"),\n        F.col(\"FULLSENSORDATA\"),\n        F.col(\"RECORDEDWHEN\"),\n        F.col(\"TEMPERATURE\"),\n        F.col(\"VEHICLEREGISTRATION\"),\n        F.col(\"VEHICLETEMPERATUREID\")\n    )\n\n\n\n# --------------------------------------------------------------------------------------------\n# --------------------------------------------- TO NEHAA ----------------------------------------------------\n# Tables in this notebook\n# Sales.CustomerTransactions\n# Sales.CustomerCategories\n# Application.People\n# Purchasing.SupplierTransactions\n# Purchasing.PurchaseOrderLines\n# Warehouse.ColdRoomTemperatures\n# Warehouse.VehicleTemperatures\n# Sales.Orders\n# Purchasing.PurchaseOrders\n# Warehouse.StockItemTransactions\n# Warehouse.StockItems\n# Warehouse.StockItemHoldings\n# Sales.Customers\n# Sales.Invoices\n# Sales.InvoiceLines\n# Sales.Orderlines\n\n\n# TO NEHAA:\n    # DO THE JOINING HERE\n    # U NEED TO EDIT THE TABLES THAT DONT EXIST\n\n    # Join Purchasing Tables\n    purchase_orders_with_details = purchasing_purchaseorders.join(\n        purchasing_suppliers,\n        purchasing_purchaseorders[\"SUPPLIERID\"] == purchasing_suppliers[\"SUPPLIERID\"]\n    ).join(\n        application_people,\n        purchasing_purchaseorders[\"CONTACTPERSONID\"] == application_people[\"PERSONID\"]\n    ).join(\n        application_deliverymethods,\n        purchasing_purchaseorders[\"DELIVERYMETHODID\"] == application_deliverymethods[\"DELIVERYMETHODID\"]\n    ).select(\n        purchasing_purchaseorders[\"PURCHASEORDERID\"],\n        purchasing_purchaseorders[\"ORDERDATE\"],\n        application_people[\"FULLNAME\"].alias(\"CONTACT_PERSON\"),\n        purchasing_suppliers[\"SUPPLIERNAME\"],\n        application_deliverymethods[\"DELIVERYMETHODNAME\"]\n    )\n\n    # Join Sales Tables\n    sales_data = sales_invoices.join(\n        sales_customers,\n        sales_invoices[\"CUSTOMERID\"] == sales_customers[\"CUSTOMERID\"]\n    ).join(\n        sales_orderlines,\n        sales_invoices[\"ORDERID\"] == sales_orderlines[\"ORDERID\"]\n    ).join(\n        application_people,\n        sales_invoices[\"CONTACTPERSONID\"] == application_people[\"PERSONID\"]\n    ).select(\n        sales_invoices[\"INVOICEID\"],\n        sales_invoices[\"INVOICEDATE\"],\n        sales_customers[\"CUSTOMERNAME\"],\n        application_people[\"FULLNAME\"].alias(\"CONTACT_PERSON\"),\n        sales_orderlines[\"DESCRIPTION\"],\n        sales_orderlines[\"QUANTITY\"],\n        sales_orderlines[\"UNITPRICE\"]\n    )\n\n    # Join Stock Tables\n    stock_with_transactions = warehouse_stockitems.join(\n        warehouse_stockitemtransactions,\n        warehouse_stockitems[\"STOCKITEMID\"] == warehouse_stockitemtransactions[\"STOCKITEMID\"]\n    ).join(\n        warehouse_colors,\n        warehouse_stockitems[\"COLORID\"] == warehouse_colors[\"COLORID\"]\n    ).select(\n        warehouse_stockitems[\"STOCKITEMNAME\"],\n        warehouse_colors[\"COLORNAME\"],\n        warehouse_stockitemtransactions[\"TRANSACTIONOCCURREDWHEN\"],\n        warehouse_stockitemtransactions[\"QUANTITY\"]\n    )\n\n    # Final Unified DataFrame\n    final_dataframe = sales_data.join(\n        stock_with_transactions,\n        sales_data[\"DESCRIPTION\"] == stock_with_transactions[\"STOCKITEMNAME\"],\n        how=\"left\"\n    #).join(\n    #    cities_with_countries,\n     #   sales_customers[\"DELIVERYCITYID\"] == cities_with_countries[\"CITYID\"],\n      #  how=\"left\"\n    ).join(\n        purchase_orders_with_details,\n        sales_data[\"INVOICEID\"] == purchase_orders_with_details[\"PURCHASEORDERID\"],\n        how=\"left\"\n    ).select(\n        sales_data[\"INVOICEID\"],\n        sales_data[\"INVOICEDATE\"],\n        sales_data[\"CUSTOMERNAME\"],\n        sales_data[\"CONTACT_PERSON\"],\n        sales_data[\"DESCRIPTION\"],\n        sales_data[\"QUANTITY\"],\n        stock_with_transactions[\"COLORNAME\"],\n        #cities_with_countries[\"CITYNAME\"].alias(\"DELIVERY_CITY\"),\n        #cities_with_countries[\"COUNTRYNAME\"].alias(\"DELIVERY_COUNTRY\"),\n        purchase_orders_with_details[\"SUPPLIERNAME\"],\n        purchase_orders_with_details[\"DELIVERYMETHODNAME\"]\n    )\n\n    final_dataframe.create_or_replace_view(\"SQL_FLATTENED_V\")\n    \n\ndef create_pos_view_stream(session):\n    session.use_schema('SNOWSQL')\n    _ = session.sql('CREATE OR REPLACE STREAM SNOWSQL_FLATTENED_V_STREAM \\\n                        ON VIEW SQL_FLATTENED_V \\\n                        SHOW_INITIAL_ROWS = TRUE').collect()\n\ndef test_pos_view(session):\n    session.use_schema('SNOWSQL')\n    tv = session.table('SQL_FLATTENED_V')\n    tv.limit(5).show()\n"
  },
  {
   "cell_type": "code",
   "id": "d9affa00-dbdc-4e86-af92-2b2c1f669c42",
   "metadata": {
    "language": "sql",
    "name": "CREATING_VIEWS",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "DROP VIEW IF EXISTS DetailedSalesTransactions;\nDROP VIEW IF EXISTS PurchasingInsights;\nDROP VIEW IF EXISTS WarehouseOverview;\nDROP VIEW IF EXISTS OrderHandling;\n\n\n\n-- HYBRID VIEW 1: DETAILED SALES TRANSACTIONS\nCREATE OR ALTER VIEW DetailedSalesTransactions AS\nSELECT \n    ct.CUSTOMERTRANSACTIONID,\n    ct.TRANSACTIONDATE,\n    ct.AMOUNTEXCLUDINGTAX,\n    ct.TAXAMOUNT,\n    ct.TRANSACTIONAMOUNT,\n    ct.OUTSTANDINGBALANCE,\n    c.CUSTOMERNAME,\n    cc.CUSTOMERCATEGORYNAME\nFROM \n    SALES_CUSTOMERTRANSACTIONS ct\nJOIN \n    SALES_CUSTOMERS c ON ct.CUSTOMERID = c.CUSTOMERID\nJOIN \n    SALES_CUSTOMERCATEGORIES cc ON c.CUSTOMERCATEGORYID = cc.CUSTOMERCATEGORYID;\n\n-- HYBRID VIEW 2: PURCHASING INSIGHTS\nCREATE OR ALTER VIEW PurchasingInsights AS\nSELECT \n    po.PURCHASEORDERID,\n    po.ORDERDATE,\n    po.DELIVERYMETHODID,\n    pol.STOCKITEMID\nFROM \n    PURCHASING_PURCHASEORDERS po\nJOIN \n    PURCHASING_PURCHASEORDERLINES pol ON po.PURCHASEORDERID = pol.PURCHASEORDERID\nJOIN \n    PURCHASING_SUPPLIERTRANSACTIONS st ON po.SUPPLIERID = st.SUPPLIERID;\n\n-- HYBRID VIEW 3: WAREHOUSE OVERVIEW\nCREATE OR ALTER VIEW WarehouseOverview AS\nSELECT \n    si.STOCKITEMID,\n    si.STOCKITEMNAME,\n    si.COLORID,\n    si.SIZE,\n    si.LEADTIMEDAYS,\n    si.QUANTITYPEROUTER,\n    sh.QUANTITYONHAND,\n    sit.TRANSACTIONOCCURREDWHEN,\n    sit.QUANTITY AS TRANSACTIONQUANTITY\nFROM \n    WAREHOUSE_STOCKITEMS si\nLEFT JOIN \n    WAREHOUSE_STOCKITEMHOLDINGS sh ON si.STOCKITEMID = sh.STOCKITEMID\nLEFT JOIN \n    WAREHOUSE_STOCKITEMTRANSACTIONS sit ON si.STOCKITEMID = sit.STOCKITEMID;\n\n-- HYBRID VIEW 4: ORDER HANDLING\nCREATE OR ALTER VIEW OrderHandling AS\nSELECT \n    o.ORDERID,\n    o.ORDERDATE,\n    o.EXPECTEDDELIVERYDATE,\n    o.CUSTOMERID,\n    o.SALESPERSONPERSONID,\n    p.FULLNAME AS SALESPERSONNAME\nFROM \n    SALES_ORDERS o\nJOIN \n    APPLICATION_PEOPLE p ON o.SALESPERSONPERSONID = p.PERSONID;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fd5b060-59da-435b-9379-1ede17d34694",
   "metadata": {
    "language": "sql",
    "name": "VIEW_DetailedSalesTransactions",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT * FROM DetailedSalesTransactions;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7277b55-d51a-4a08-b07a-415b228704d1",
   "metadata": {
    "language": "sql",
    "name": "VIEW_PurchasingInsights",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT * FROM PurchasingInsights;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "144fc4f0-dd23-4815-a2e5-61670e42ae8d",
   "metadata": {
    "language": "sql",
    "name": "VIEW_WarehouseOverview",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT * FROM WarehouseOverview;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75f4ffb5-0d98-454d-be7f-d4f6beb80411",
   "metadata": {
    "language": "sql",
    "name": "VIEW_OrderHandling",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT * FROM OrderHandling;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60002c21-9624-4515-a01c-4a5309337f2a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell12",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# use this place to validate the tables\n",
    "# test the code\n",
    "create_pos_view(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d69cee-46c8-4637-b48a-2df7464a7cf6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell13",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "# Test the view created by create_pos_view\n",
    "def validate_view(session):\n",
    "    session.use_schema('SNOWSQL')\n",
    "    # Load the view into a Snowpark DataFrame\n",
    "    df = session.table(\"SQL_FLATTENED_V\")\n",
    "    # Display the first few rows for validation\n",
    "    df.show()\n",
    "\n",
    "validate_view(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abbe21-1f8f-45d6-9f44-d0b4b90ba312",
   "metadata": {
    "name": "cell16",
    "collapsed": false,
    "resultHeight": 179
   },
   "source": "## Data UDF\n\nDuring this step we will be creating and deploying our first Snowpark Python object to Snowflake, a user-defined function (or UDF). To begin with the UDF will be very basic, but in a future step we'll update it to include a third-party Python package. Also in this step you will be introduced to the new SnowCLI, a new developer command line tool. SnowCLI makes building and deploying Snowpark Python objects to Snowflake a consistent experience for the developer. More details below on SnowCLI."
  },
  {
   "cell_type": "markdown",
   "id": "26e6c6c7-e3b3-4349-9acf-1160b660513d",
   "metadata": {
    "name": "cell17",
    "collapsed": false,
    "resultHeight": 217
   },
   "source": "## Orders Update Sproc\n\nDuring this step we will be creating and deploying our first Snowpark Python stored procedure (or sproc) to Snowflake. This sproc will merge changes from the `HARMONIZED.POS_FLATTENED_V_STREAM` stream into our target `HARMONIZED.ORDERS` table.\n\n### Running the Sproc Locally\nTo test the procedure locally, you will execute the following script."
  },
  {
   "cell_type": "code",
   "id": "67e3c214-155f-4a0a-8992-58b75e97ac57",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d4fc6-e894-46e7-be6e-40b2b916dfb2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# close session\nsession.close()"
  }
 ]
}